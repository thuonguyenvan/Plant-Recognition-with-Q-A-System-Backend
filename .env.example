# ============================================================================
# ENVIRONMENT CONFIGURATION - EXAMPLE
# ============================================================================
# Copy this file to .env and fill in your actual values
# Command: cp .env.example .env
# ============================================================================

# ----------------------------------------------------------------------------
# SUPABASE DATABASE CONFIGURATION
# ----------------------------------------------------------------------------
# Get these from your Supabase project settings:
# https://app.supabase.com/project/_/settings/api

SUPABASE_URL=https://your-project-id.supabase.co
SUPABASE_KEY=your_supabase_anon_key_here

# Direct Database Connection (for scripts/fast_import.py)
# Format: postgresql://postgres.[PROJECT-REF]:[PASSWORD]@aws-0-[REGION].pooler.supabase.com:6543/postgres
SUPABASE_DB_URI=postgresql://postgres.your-project-ref:your-password@aws-0-region.pooler.supabase.com:6543/postgres

# ----------------------------------------------------------------------------
# MEGALLM API CONFIGURATION
# ----------------------------------------------------------------------------
# Get API key from: https://ai.megallm.io

MEGLLM_API_KEY=your_megallm_api_key_here
MEGLLM_BASE_URL=https://ai.megallm.io/v1
MEGLLM_MODEL=qwen/qwen3-next-80b-a3b-instruct

# ----------------------------------------------------------------------------
# COMPUTER VISION API (Plant Classification)
# ----------------------------------------------------------------------------
# URL to your deployed plant classification model API
# Example: Hugging Face Space endpoint

CV_API_URL=https://your-cv-api-url-here/predict

# ----------------------------------------------------------------------------
# EMBEDDING MODEL CONFIGURATION
# ----------------------------------------------------------------------------
# Hugging Face model for Vietnamese text embeddings

EMBEDDING_MODEL=keepitreal/vietnamese-embedding
EMBEDDING_DIMENSION=1024

# ----------------------------------------------------------------------------
# APPLICATION SETTINGS
# ----------------------------------------------------------------------------

# Environment: development, production, staging
ENV=production

# Debug mode: true or false
DEBUG=false

# Server configuration
HOST=0.0.0.0
PORT=8000

# CORS origins (comma-separated for multiple origins)
ALLOWED_ORIGINS=http://localhost:3000,https://your-frontend-domain.com

# ----------------------------------------------------------------------------
# OPTIONAL SETTINGS
# ----------------------------------------------------------------------------

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Cache TTL in seconds
CACHE_TTL=3600

# Max tokens for LLM responses
MAX_TOKENS=2048

# Temperature for LLM (0.0-2.0)
TEMPERATURE=0.7
